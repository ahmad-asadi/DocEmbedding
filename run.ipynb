{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code to train Word embeddings on given corpus.\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "from  probarray import ProbArray\n",
    "from  compatibility import range, pickle\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "\n",
    "# CONFIG_PATH = sys.argv[1]\n",
    "DATA_PATH =  (\"data\")\n",
    "\n",
    "config_file = open(\"config.json\", 'r')\n",
    "CONFIG = json.load(config_file)\n",
    "backend = CONFIG['backend']\n",
    "# print(backend)\n",
    "contextSize = CONFIG['contextSize']\n",
    "min_count = CONFIG['min_count']\n",
    "lr = CONFIG['lr']\n",
    "ntimes = CONFIG['ntimes']\n",
    "newdims = CONFIG['newdims']\n",
    "maxnum = CONFIG['maxnum']\n",
    "# import theano/tensorflow depending on the backend set by user\n",
    "if backend == 'tf':\n",
    "    from backprop_tf import TrainModel\n",
    "# elif backend == 'th':\n",
    "#     from embedding.backprop_th import TrainModel\n",
    "\n",
    "\n",
    "def train(folder):\n",
    "    '''\n",
    "    Function to train autoencoder.\n",
    "    '''\n",
    "    t = time.time()\n",
    "    lr_decay = 0.95\n",
    "    pa = ProbArray()\n",
    "    # Frequency to filter out low freq words\n",
    "    freq = {}\n",
    "    files = [ f for f in os.listdir(folder) if os.path.isfile(f)]\n",
    "    filepaths = list(map(lambda x: folder + \"/\" + x, files ))\n",
    "    rgx = re.compile(\"([\\w][\\w']*\\w)\")\n",
    "    # Another iterator to count frequency of words\n",
    "    print (\"Pre-processing (clearning garbage words)\")\n",
    "    for filepath in filepaths:\n",
    "        # text = open(filepath).read().lower()\n",
    "        text = open(filepath).read().lower()\n",
    "        tokens = re.findall(rgx, text)\n",
    "\n",
    "        N = len(tokens)\n",
    "        for i in range(0,N):\n",
    "            if tokens[i] in freq:\n",
    "                freq[tokens[i]] += 1\n",
    "            else:\n",
    "                freq[tokens[i]] = 1\n",
    "\n",
    "    # Sort the frequencies storing in (freq, token) pairs and prune words with freq < min_count\n",
    "    tokenFreq = sorted(freq.items(), key = lambda x: x[1])\n",
    "    garbageWords = []\n",
    "    for item in tokenFreq:\n",
    "        if item[1] < min_count:\n",
    "            garbageWords.append(item[0])\n",
    "\n",
    "    print (\"Generating co-occurence matrix\")\n",
    "    doc_text = \"\"\n",
    "    for filepath in filepaths:\n",
    "        text = open(filepath).read().lower()\n",
    "        words = re.findall(rgx, text)\n",
    "        N = len(words)\n",
    "        temp = [' '] * (N +  contextSize)\n",
    "        temp[contextSize : (contextSize + N)] = words\n",
    "        words = temp\n",
    "        for i in range(contextSize, (contextSize + N)):\n",
    "            # Filter out garbage words\"\n",
    "            #if words[i] not in garbageWords:\n",
    "            # Include context size specified by user\n",
    "            for j in range(i-contextSize, i):\n",
    "                if words[i] != ' ' and words[j] != ' ':\n",
    "                        pa.addcontext(words[j], words[i])\n",
    "                        pa.addcontext(words[i], words[j])\n",
    "\n",
    "    print (\"Co-occurence matrix generated\")\n",
    "    print (\"Starting training\")\n",
    "    tm = TrainModel(maxnum, newdims, lr)\n",
    "    pa.freeze()\n",
    "    tm.trainonone(pa, ntimes)\n",
    "    #lr /=float(1+k*lr_decay)\n",
    "\n",
    "    wordembeddings = {}\n",
    "    for numwordvec in pa.getallwordvecs():\n",
    "        (num, wordvec) = numwordvec\n",
    "        if num % 1000 == 0:\n",
    "            print(num)\n",
    "        word = pa.wordnumrelation.getWord(num)\n",
    "        if backend == 'tf':\n",
    "            embedding = tm.getoutput(wordvec, './embedding.chk')\n",
    "        else:\n",
    "            embedding = tm.getoutput(wordvec)\n",
    "        wordembeddings[word] = embedding\n",
    "\n",
    "    print (\"Training proces done, dumping embedding into persistant storage!\")\n",
    "\n",
    "    with open(r'./embeddings.pickle', \"wb\") as outfile:\n",
    "        pickle.dump(wordembeddings, outfile)\n",
    "    print (\"Training completed! Embedding done.\")\n",
    "    print (\"time is %f\" % (time.time()-t))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(DATA_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
